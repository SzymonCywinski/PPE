---
title: "Prognozowanie: zarys teorii :-)"
author: "TP"
date: "28/11/2020"
output: html_document
   
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Wstęp

Wczytanie danych/wykresy/ocena ogólna rozwoju zjawiska

```
## Wczytanie danych
z <- read.csv("MZM.csv", dec=".", sep = ';',  header=T, na.string="NA");
## Co jest w pliku MZM.csv
str(z)
## albo w lewym/górnym oknie RStudio (zakładka Environment)
##
is.ts(z)

## zamiana zmiennej razem z ramki z na szereg czasowy
## częstotliwość miesięczna pierwsza obserwacja 2015/1
z <-ts(z$razem, start=c(2015, 1), frequency=12)

## sprawdzenie czy z jest typu TS
is.ts(z)

## jaka jest częstotliwość/początek/koniec
frequency(z)
start(z)
end(z)
## wykres
plot(z)
## wykres sezonowości
seasonplot(z, year.labels = TRUE, main="wykres sezonowy")
## statystyki zbiorcze
summary(ts)

# "wycięcie" fragmentu od 4 kwartału 2016
z1 <- window (z, start=c(2016, 4))
```

## Składniki szeregu czasowego

W szeregu czasowym można zwykle wyróżnić długookresową tendencję (trend);
powtarzalne wahania (sezonowość); resztę traktuje się jako wartości
przypadkowe. Reasumując:

$$TS = T + S + E$$

lub 

$$TS = T \cdot S \cdot E$$

Pierwszy wariant nazywa się **addytywny** drugi **multiplikatywny**. 
W wariancie addytywnym zmiany
(trendu/sezonowości) okres/okres są stałe; w wariancie multiplikatywnym **tempo zmiany** jest stałe, tj. zjawisko okres/okres rośnie/spada o x%. W jednostkach bezwzględnych
oznacza to, że rośnie/spada coraz szybciej.

## Ocena modelu

### Ocena jakości dopasowania

* reszta (*error*) to różnica między wartością obserwowaną ($y_i$)
a wartością wyznaczoną z modelu $\hat y_i$,  czyli $e_i = y_i - \hat y_i$ 

* MSE -- średnia z kwadratów reszt (*mean square error*)

* RMSE -- pierwiastek kwadratowy z MSE (*root mean square error*)
     
* błąd procentowy -- udział reszty w wartości obserwowanej $p_i = e_i/y_i \cdot 100$
     
* MAPE -- średni bezwględny błąd procentowy (suma wartości bezwzględnych $p_i$)

UWAGA: MAPE ma fundamentalną wadę, mianowicie jeżeli $y_i$ jest bliskie zero,
to $p_i$ jest albo nieskończonością albo jest absurdalnie duże

* wielkość zbioru testowego to zwykle 20% całości (a zbiór uczący 80%). Alternatywnie
wielkość zbióru testowego jest równa horyzontowi prognozy.

* zawsze można dopasować model perfekcyjnie, ale
model dopasowany perfekcyjnie do danych niekoniecznie będzie dobrze przewidywał
(*over-fitting* czyli **przeuczenie**)

### Ocena reszt
     
Wymagane jest aby:

* reszty były nieskorelowane (jeżeli są to prognoza nie wykorzystuje
     całej dostępnej informacji, innymi słowy można ją poprawić)

* średnia reszt jest równa zero (są **nieobciążone**, nie ma błędu
systematycznego zwanego **obciążeniem**)
     
Pożądane jest aby:
     
* wariancja reszt była stała

* rozkład reszt był normalny

powyższe pozwala konstruuować lepsze prognozy przedziałowe (od/do)

```
## wyznaczenie reszt estimated.model to obiekt R
## powstały w wyniku zastosowania funkcji prognozowania np. ses
## tj fit <- ses (...)
res <- residuals (fit)
     
## wizualna ocena nieobciążoności/stałości wariancji
plot (res, main='tytuł-wykresu', xlab='ośX', ylab='ośY')
```
na tym wykresie reszty powinny przypadkowo oscylować wokół zera. Jeżeli zaś są systematycznie
powyżej/poniżej to świadczy to o tym że są **obciążone** Jeżeli systematycznie rosną/maleją, to nie mają stałej wariancji

```
## wizualna ocena nieskorelowania
## funkcja autokorelacji
Acf(res, main='...')
```

wykres ten posiada dwie poziome linie określające maksymalną akceptowaną wielkość współczynnika korelacji, którą
można uznać za przypadkową 
a nie wynikające z tego, że składnik losowy z dwóch okresów jest istotnie skorelowany. Słupki dla kolejnych
opóźnień powinny być poniżej/powyżej tych linii

```
## wizualna ocena normalności reszt
hist(res)
```

Kształt histogramu powinien być zbliżony do normalnego, w szczególności winien być
zbliżony do symetrycznego. Asymetria świadczy że rozkład reszt **nie jest normalny**.


Oprócz metod *na oko* można użyć testów.
Idea testów statystycznych jest następująca: stawiamy pewną (hipo)tezę, którą 
**weryfikujemy** używając do tego stosownego zbioru danych.

Prosty przykład: rzucamy monetą i stawiamy hipotezę że jest symetryczna co by oznaczało że
prawdopodobieństwo wyrzucenia orła albo reszki jest jednakowe. Rzucamy monetą, wypadło orzeł,
czy można coś powiedzieć o symetryczności monety? Rzucamy dwa razy wypadł w obu przypadkach orzeł. 
Takie coś się zdarza  1 raz na 4, więc także niespecjalnie rzadko. Rzucamy dalej. W 12 próbach
orzeł wypadł 11 razy. Takie coś się zdarza 12 razy na $2^{12}$ prób czyli około dwa razy na 100 prób. 
Sprawa zaczyna być podejrzana, że moneta jest faktycznie symetryczna.

Wyznaczamy z góry i arbitralnie prawdopodobieństwo popełnienia błędu przy weryfikowaniu naszej (hipo)tezy.
Np 5% (co oznacza że przeciętnie mylimy się 5 razy na 100). Przy 5% błędzie eksperyment z 12 rzutami
wskazał, że moneta jest niesymetryczna, bo zdarzenie 
pn. *wypadło 11 orłów* zdarza się raz na 50 razy czyli około 2% (przypomnę, 
że zdarzenie dwa orły na dwa rzuty to aż 25%.)

Czy można powiedzieć rzucając raz czy dwa razy że moneta jest symetryczna? No nie bardzo. Raczej można powiedzieć
że **nic nie wskazuje że jest niesymetryczna** a to co innego.

Wracając do 11 orłów. Prawdopodobieństwo które założyliśmy to prawdopodobieństwo zdarzenia że *moneta
jest symetryczna* a mimo to wypadło 11 orłów na 12 prób. Czyli monetę zdyskwalifikowaliśmy niesłusznie. 
To się nazywa *false-positive* (uznanie za błąd czegoś co błędem nie jest) 
albo błąd pierwszego rodzaju (*type I error*). 

Ale jest jeszcze drugi możliwy błąd zwany *false-negative* (uznanie za prawdę czegoś co jest błędne, błąd 
drugiego rodzaju albo *type II error*). **Prawdopodobieństwa popełnienia tego błędu
nie określilśmy** zatem nie można powiedzieć **moneta jest symetryczna** jeżeli eksperyment zakończył się
wynikiem z prawdopodobieństwem mniejszym od zakładanego (dla statystyka takie 
twierdzenie zawsze będzie **prawdą z określonym błędem** a tutaj nie ma tego określonego błędu)

Reasumując jeżeli w wyniku eksperymentu otrzymamy wynik mało prawdopodobny z punktu widzenia zakładanej
hipotezy to odrzucamy ją a jak nie to **nie mamy podstaw do odrzucenia** (co nie oznacza, że przyjmujemy).

W nauce generalnie wykazanie fałszywości (**falsyfikacja) jest bardziej pewna niż wykazanie prawdziwości.**

Jedynie hipotezy sfalsyfikowane są naukowe (prawdziwe).
Nauka rozwija się dzięki kolejnemu odrzucaniu sfalsyfikowanych teorii. Indukcja to lipa (BTW)

Nawet nie wiedząc niczego więcej można interpretować z mądrą miną **każdy test statystyczny** ideą których jest
podjęcie decyzji o pewnej hipotezie na podstawie wyniku eksperymentu. Jeżeli ten wynik jest mało prawdopodobny
to hipotezę należy odrzucić co oznacza przyjęcie za prawdę **hipotezy alternatywnej**, która zwykle jest
jej zaprzeczeniem.  Czyli jeżeli hipoteza mówi: w modelu nie ma AC, to hipoteza alternatywna mówi: 
w modelu jest AC. Już Rzymianie wiedzieli że tertium-non-datur...

Każdy współczesny program statystyczny czy arkusz kalkulacyjny podaje owo prawdopodobieństwo. Jeżeli
jest ono małe to odrzucamy hipotezę a jeżeli nie jest małe to nie ma podstaw do odrzucenia.

Hipotezę nazywamy hipotezą zerową ($H_0$), hipotezę przeciwną do zerowej nazywamy alternatywną ($H_1$),
przyjętą jako minimalna do podjęcia 
decyzji wartość prawdopodobieństwo nazywamy poziomem istotności (*significance*)

```
## albo test Boxa-Ljunga, małe wartości p świadczą o AC
Box.test(res, type='Ljung-Box')
```

Jeżeli w teście Boxa-Ljunga otrzymaliśmy małą wartość $p$ to występuje autokorelacja składnika losowego.
Jest to sygnał że być może należy użyć innej metody (modelu); albo mając do wyboru model 
z AC i bez AC (formalnie taki w którym nie wykazano istnienia AC) należy wybrać ten drugi.

## Uproszczona procedura wyboru modelu

* oszacować kilka konkurencyjnych modeli;

* wybrać ten, dla którego dopasowanie **dla zbioru testowego** jest najlepsze, 
 **reszty są nieobciążone i nieskorelowane**; 

* powtórzyć oszacowanie dla całości dostępnych danych

## Wybrane metody prognozowania

Metod jest *multum*. Poniżej trzy klasyczne podejścia.

## Prosty model trendu 

Zjawisko ma charakter liniowy. Prognozowanie polega na wyznaczeniu
prostej najlepiej dopasowanej do danych. Jeżeli występuje trend
zakłada się że amplituda wahań sezonowych jest stała (co do wartości bezwględnych (addytywne)lub
względnych (multiplikatywne))

```
fit <- tslm(razem.learn ~ trend )
## z sezonowością
fit <- tslm(razem.learn ~ trend + season )

## a potem
summary(fit)
```

## Wygładzanie wykładnicze 
     
proste wygładzanie wykładnicze (*simple exponential smoothing*)
**Szereg nie wykazuje się trendem/sezonowością**

Prognozowanie na podstawie ostatniej wartości:

$$y_{T+h} = y_T$$

Prognozowanie na podstawie  średniej z wartości empirycznych

$$y_{T+h} = y_T$$

SES

$$y_{T+1} = a Y_T + a(a-1)^1 Y_{T-1} + a(a-1)^2 Y_{T-2} + \cdots$$

można powiedzieć że $y_{T+1}$ to ważona średnia z wartości empirycznych. Jak bardzo
ważona zależy od parametru $a$. Im $a$ jest bliższe zeru tym szybciej
wartość wag spada (poetycko się mówi: pamięć modelu jest krótsza)

```
## ses
fit <- ses(x, h=, alpha=)

## initial określenie sposobu inicjalizacji algorytmu 
fit <- ses(x, alpha=a, initial='simple', h=3)

## Holt trend bez sezonowości
fit <- holt(x, alpha=a, beta=b, initial='simple', h=3)

## Holt-Winters trend/sezonowość
fit <- hw(x, seasonal='additive', h=3)
fit <- hw(x, seasonal='multiplicative', h=3)

## albo
## Funkcja automatycznie wybierająca
## najlepszy wariant ES
fit <- ets(x, h=)

## dalej `jedziemy' tak:
plot(fit)
summary(fit)
checkresiduals(fit)
```

## ARIMA
     
### Stacjonarność 

Szereg czasowy którego właściwości nie zmieniają się nazywa się
*stacjonarnym*. Występowanie trendu lub sezonowości oznacza brak
stacjonarności

### Różnicowanie
     
Różnicowanie pozwala wyeliminować trend/sezonowość:

$$y_t' = y_t - y_{t-1}$$
     
można różnicować różnicowane:

$$y_t'' = y_t' - y_{t-1}'$$

Różnicowanie skraca szereg czasowy (o jeden okres)

W przypadku sezonowości różnicujemy o liczbę okresów
sezonowości (np. $m=12$ dla danych miesięcznych)

$$y_t' = y_t - y_{t-m}$$

```
## Do różnicowania używamy funkcji lag
lag <-12
lagged_x <- diff(x, lag)
```

### Modele autoregresyjne (AR)

bieżąca wartość jest średnią (liniową kombinacją) z wartości
poprzednich (albo regresja liniowa na wartościach opóźnionych)

$$y_t = c_0 + c_1 y_{t-1} + c_2 y_{t-2}+ ... + c_p y_{t-p} + e_t$$

założenie: stacjonarność 

### Modele średniej ruchomej (MA)

bieżąca wartość jest średnią (liniową kombinacją) z błędów dla
wartości opóźnionych:
     
$$y_t = c_0 + c_1 e_{t-1} + c_2 e_{t-2}+ ... + e_p y_{t-q}$$

założenie: stacjonarność

ARIMA to AR + MA albo dokładniej ARIMA(p, d, q) to AR(p) +
MA(q) gdzie $d$ oznacza rząd różnicowania

```
## oszacowanie modelu w R
fit <- Arima(x, order=c(p, d, q) )
## czyli Arima(x, c(0,0,q)) = MA(q)
## czyli Arima(x, c(p,0,0)) = AR(p)
##
## funkcja auto.arima wybiera sam najlepszy model
fit <- auto.arima(x)
```

